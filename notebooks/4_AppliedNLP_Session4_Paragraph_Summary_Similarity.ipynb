{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275b920e",
   "metadata": {},
   "source": [
    "# Session 4 — Paragraph-Level Analysis\n",
    "## Measure 4: Paragraph–Summary Similarity (Compressibility)\n",
    "\n",
    "### What is \"Compressibility\"?\n",
    "\n",
    "Imagine you have to summarize a paragraph in one sentence. Some paragraphs are easy to summarize because they focus on one main idea. Others are harder because they jump between multiple topics or include lots of details.\n",
    "\n",
    "**Compressibility** measures how well a short summary captures the essence of the full paragraph:\n",
    "- **High compressibility** (high similarity): The summary is very similar to the full paragraph → easy to compress\n",
    "- **Low compressibility** (low similarity): The summary misses a lot → hard to compress\n",
    "\n",
    "**Real-world analogy:**\n",
    "- Easy to summarize: *\"The cat sat on the mat. The furry feline rested comfortably on the soft rug.\"* → Both sentences say the same thing\n",
    "- Hard to summarize: *\"Alice saw a rabbit. The sky was blue. She felt hungry. A clock chimed.\"* → Each sentence is different!\n",
    "\n",
    "### What This Notebook Does:\n",
    "\n",
    "**Step 1**: Create simple summaries\n",
    "- For each paragraph, we pick the **longest sentence** as our \"summary\"\n",
    "- This is a simple trick - longer sentences often contain more key information\n",
    "\n",
    "**Step 2**: Convert text to numbers (embeddings)\n",
    "- We use **MiniLM**, a small AI model, to convert both the paragraph and summary into numerical vectors\n",
    "- These vectors capture the **meaning** of the text (not just words)\n",
    "- Similar meanings = vectors point in similar directions\n",
    "\n",
    "**Step 3**: Measure similarity\n",
    "- We use **cosine similarity** to compare the paragraph vector with its summary vector\n",
    "- Score close to 1.0 = very similar (highly compressible)\n",
    "- Score close to 0.0 = very different (not compressible)\n",
    "\n",
    "**Step 4**: Analyze patterns\n",
    "- Which paragraphs are easy vs. hard to summarize?\n",
    "- Does paragraph length affect compressibility?\n",
    "- How do the two Alice books compare?\n",
    "\n",
    "### Why This Matters for Modern AI:\n",
    "\n",
    "**ChatGPT and Token Limits:**\n",
    "When you chat with ChatGPT, there's a limit to how much text it can \"remember\" at once (the context window). To fit long documents, AI systems must **compress** information:\n",
    "\n",
    "1. **Chunking Documents**: \n",
    "   - Systems split long documents into paragraphs/chunks\n",
    "   - They want chunks that are **highly compressible** (main idea is clear)\n",
    "   - Low-compressibility chunks might get split further or handled specially\n",
    "\n",
    "2. **RAG Systems** (Retrieval-Augmented Generation):\n",
    "   - When searching documents to answer questions, RAG prefers well-structured, compressible paragraphs\n",
    "   - These paragraphs have clear main ideas that are easy to extract and use\n",
    "\n",
    "3. **Summarization Quality**:\n",
    "   - High paragraph-summary similarity means the paragraph is **well-focused**\n",
    "   - This makes automatic summarization more accurate\n",
    "   - AI can confidently extract key points\n",
    "\n",
    "4. **Practical Example**:\n",
    "   - **Good paragraph** (high compressibility): *\"Climate change is accelerating. Rising temperatures cause ice to melt. This leads to sea level rise.\"* → All sentences connect to one theme\n",
    "   - **Poor paragraph** (low compressibility): *\"Climate change is accelerating. Alice likes tea. The rabbit was late.\"* → Multiple unrelated ideas\n",
    "\n",
    "This notebook shows you which paragraphs in Carroll's work are well-focused and which jump between ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49340556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# You may need to install this once in your environment:\n",
    "# !pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def load_book(filepath: str) -> str:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    if 'CHAPTER I' in text:\n",
    "        start = text.find('CHAPTER I')\n",
    "        text = text[start:]\n",
    "    elif '*** START OF' in text:\n",
    "        start = text.find('*** START OF')\n",
    "        text = text[start + 100:]\n",
    "\n",
    "    if '*** END OF' in text:\n",
    "        end = text.find('*** END OF')\n",
    "        text = text[:end]\n",
    "    elif 'End of Project Gutenberg' in text:\n",
    "        end = text.find('End of Project Gutenberg')\n",
    "        text = text[:end]\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "wonderland_text = load_book('../data/Wonderland.txt')\n",
    "looking_glass_text = load_book('../data/Looking-Glass.txt')\n",
    "\n",
    "print(f\"Wonderland characters: {len(wonderland_text):,}\")\n",
    "print(f\"Looking-Glass characters: {len(looking_glass_text):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2001814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text: str, min_words: int = 10) -> List[str]:\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    raw_paras = re.split(r'\\n\\s*\\n+', text)\n",
    "    paras = []\n",
    "    for p in raw_paras:\n",
    "        cleaned = re.sub(r'\\s+', ' ', p).strip()\n",
    "        if not cleaned:\n",
    "            continue\n",
    "        if len(cleaned.split()) < min_words:\n",
    "            continue\n",
    "        paras.append(cleaned)\n",
    "    return paras\n",
    "\n",
    "def simple_paragraph_summary(paragraph: str, max_sentences: int = 1) -> str:\n",
    "    \"\"\"Very simple extractive summary: choose the longest sentence(s)\n",
    "    as a cheap proxy for importance.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'[.!?]+\\s+', paragraph.strip())\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if not sentences:\n",
    "        return ''\n",
    "    ranked = sorted(sentences, key=lambda s: len(s.split()), reverse=True)\n",
    "    return ' '.join(ranked[:max_sentences])\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    if b.ndim > 1:\n",
    "        b = b.reshape(-1)\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def summary_similarity_embeddings(paragraphs: List[str], max_sentences: int = 1) -> Tuple[list, list]:\n",
    "    sims = []\n",
    "    lengths = []\n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) < 5:\n",
    "            continue\n",
    "        summary = simple_paragraph_summary(p, max_sentences=max_sentences)\n",
    "        if not summary:\n",
    "            continue\n",
    "        para_emb = model.encode(p)\n",
    "        sum_emb = model.encode(summary)\n",
    "        sim = cosine_similarity(para_emb, sum_emb)\n",
    "        sims.append(sim)\n",
    "        lengths.append(len(p.split()))\n",
    "    return sims, lengths\n",
    "\n",
    "wonderland_paras = split_into_paragraphs(wonderland_text)\n",
    "looking_glass_paras = split_into_paragraphs(looking_glass_text)\n",
    "\n",
    "w_sim, w_len = summary_similarity_embeddings(wonderland_paras)\n",
    "g_sim, g_len = summary_similarity_embeddings(looking_glass_paras)\n",
    "\n",
    "print(f\"Wonderland mean summary similarity (MiniLM): {sum(w_sim)/len(w_sim):.3f}\")\n",
    "print(f\"Looking-Glass mean summary similarity (MiniLM): {sum(g_sim)/len(g_sim):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536668d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "axes[0].scatter(w_len, w_sim, alpha=0.5)\n",
    "axes[0].set_title('Wonderland')\n",
    "axes[0].set_xlabel('Paragraph length (words)')\n",
    "axes[0].set_ylabel('Summary similarity (cosine, MiniLM)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(g_len, g_sim, alpha=0.5)\n",
    "axes[1].set_title('Looking-Glass')\n",
    "axes[1].set_xlabel('Paragraph length (words)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd286115",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(w_sim, bins=20, alpha=0.6, label='Wonderland', density=True)\n",
    "ax.hist(g_sim, bins=20, alpha=0.6, label='Looking-Glass', density=True)\n",
    "ax.set_xlabel('Similarity between paragraph and simple summary (cosine, MiniLM)')\n",
    "ax.set_ylabel('Density (normalized)')\n",
    "ax.set_title('How Compressible Are Paragraphs? (Higher = easier to summarize)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513bfbd",
   "metadata": {},
   "source": [
    "### How to Interpret These Results\n",
    "\n",
    "#### Scatter Plots (Similarity vs. Paragraph Length)\n",
    "\n",
    "**What you're seeing:**\n",
    "- Each dot = one paragraph\n",
    "- **X-axis**: Paragraph length in words\n",
    "- **Y-axis**: Similarity score (0.0 to 1.0)\n",
    "\n",
    "**Key Patterns:**\n",
    "\n",
    "1. **Very High Scores (0.9-1.0)**:\n",
    "   - Most paragraphs cluster in this range!\n",
    "   - This means the longest sentence usually captures the paragraph's essence very well\n",
    "   - **Why?** In narrative fiction, paragraphs tend to focus on one scene/moment\n",
    "   - The longest sentence often describes the main action or idea\n",
    "\n",
    "2. **Slight Downward Trend**:\n",
    "   - Longer paragraphs show slightly more variation\n",
    "   - Some drop to 0.7-0.8 range\n",
    "   - **Why?** Longer paragraphs may include:\n",
    "     - Multiple sub-topics (action + dialogue + description)\n",
    "     - Tangential details that the summary misses\n",
    "     - Complex narrative threads\n",
    "\n",
    "3. **Few Low Outliers (below 0.7)**:\n",
    "   - These are paragraphs where the longest sentence is NOT representative\n",
    "   - Could be:\n",
    "     - Dialogue-heavy paragraphs (longest sentence ≠ main point)\n",
    "     - Lists or enumerations\n",
    "     - Rapid scene changes\n",
    "\n",
    "**Comparison Between Books:**\n",
    "- Both show nearly identical patterns\n",
    "- Mean similarity around 0.90-0.92\n",
    "- This confirms Carroll's consistent, focused writing style\n",
    "\n",
    "---\n",
    "\n",
    "#### Histogram (Distribution of Compressibility)\n",
    "\n",
    "**What you're seeing:**\n",
    "- Shows how common each similarity range is\n",
    "- Peak around 0.95+ means most paragraphs are HIGHLY compressible\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Sharp Peak at High Values (0.90-1.0)**:\n",
    "   - The vast majority of paragraphs score 0.90+\n",
    "   - **Interpretation**: Carroll's paragraphs are well-focused and cohesive\n",
    "   - Each paragraph generally discusses one clear topic/scene\n",
    "   - This makes his writing easy to read and follow\n",
    "\n",
    "2. **Very Few Low Values**:\n",
    "   - Almost no paragraphs below 0.70\n",
    "   - This is typical of good narrative writing\n",
    "   - Professional authors maintain topic coherence within paragraphs\n",
    "\n",
    "3. **Identical Distributions**:\n",
    "   - Both books overlap almost perfectly\n",
    "   - Shows Carroll maintained consistent quality across both works\n",
    "\n",
    "---\n",
    "\n",
    "### What This Tells Us About Lewis Carroll's Writing:\n",
    "\n",
    "1. **Highly Cohesive Paragraphs**: \n",
    "   - Each paragraph focuses on one clear idea/scene\n",
    "   - This makes the text easy to follow for young readers\n",
    "\n",
    "2. **Strong Sentence Structure**:\n",
    "   - The longest sentences effectively capture paragraph essence\n",
    "   - Shows careful, deliberate writing (not rambling)\n",
    "\n",
    "3. **Good Summarization Target**:\n",
    "   - These texts would be relatively easy for AI systems to summarize\n",
    "   - High compressibility = clear extraction of key points\n",
    "\n",
    "4. **RAG-Friendly**:\n",
    "   - If you were building a search system over these books, paragraphs would work well as chunks\n",
    "   - Each chunk has a clear, extractable main idea\n",
    "\n",
    "---\n",
    "\n",
    "### Comparing to Other Text Types:\n",
    "\n",
    "**What would we see in different genres?**\n",
    "\n",
    "- **Academic papers**: Similar high scores (0.85-0.95) - each paragraph makes one clear point\n",
    "- **Stream-of-consciousness fiction**: Lower scores (0.60-0.80) - paragraphs jump between thoughts\n",
    "- **Technical manuals**: Very high scores (0.90-0.98) - extremely focused, one-topic paragraphs\n",
    "- **Social media posts**: Wide variation (0.40-0.95) - inconsistent structure\n",
    "- **News articles**: High scores (0.85-0.95) - inverted pyramid structure, clear focus\n",
    "\n",
    "Carroll's scores (0.90-0.95) place him solidly in the \"well-structured narrative\" category!\n",
    "\n",
    "---\n",
    "\n",
    "### Application to Your Projects:\n",
    "\n",
    "**If you're working with documents:**\n",
    "- Use compressibility scores to identify well-written vs. poorly-structured paragraphs\n",
    "- Filter out low-compressibility chunks before feeding to AI systems\n",
    "- Identify sections that need editing (low scores = unfocused writing)\n",
    "\n",
    "**If you're building RAG/search systems:**\n",
    "- Prioritize high-compressibility paragraphs in search results\n",
    "- They're more likely to contain clear, useful information\n",
    "- Split or flag low-compressibility paragraphs for special handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd469ce",
   "metadata": {},
   "source": [
    "## Memory Cleanup\n",
    "\n",
    "If you're running low on memory, run this cell to free up RAM by deleting large variables and clearing the model cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556363a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete large variables to free memory\n",
    "del wonderland_text, looking_glass_text\n",
    "del wonderland_paras, looking_glass_paras\n",
    "del w_sim, w_len, g_sim, g_len\n",
    "\n",
    "# Clear matplotlib figures\n",
    "plt.close('all')\n",
    "\n",
    "# Unload the model from memory\n",
    "del model\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory cleaned! Large variables deleted and garbage collected.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
