{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb4aa94",
   "metadata": {},
   "source": [
    "# Session 4 — Paragraph-Level Analysis\n",
    "## Measure 3: Discourse Marker Density per Paragraph\n",
    "\n",
    "### What are Discourse Markers?\n",
    "\n",
    "**Discourse markers** are special words or phrases that show how sentences connect to each other. Think of them as **road signs in writing** - they tell readers what's coming next and how it relates to what came before.\n",
    "\n",
    "**Common examples:**\n",
    "- **\"However\"** → signals that something contrasts with what was just said\n",
    "  - *\"I love coffee. However, I can't drink it at night.\"*\n",
    "- **\"Therefore\"** → shows cause and effect\n",
    "  - *\"It's raining. Therefore, we'll stay inside.\"*\n",
    "- **\"For example\"** → introduces a specific instance\n",
    "  - *\"Many animals hibernate. For example, bears sleep through winter.\"*\n",
    "- **\"Meanwhile\"** → indicates something happening at the same time\n",
    "  - *\"Alice explored Wonderland. Meanwhile, her sister read quietly.\"*\n",
    "\n",
    "### Why Count Discourse Markers?\n",
    "\n",
    "**The Simple Idea:**\n",
    "Some writers use lots of these signpost words (formal, academic writing), while others use very few (casual conversation, fiction). By counting them, we can:\n",
    "1. Understand an author's writing style\n",
    "2. Compare different texts or genres\n",
    "3. Identify which parts of a book are more structured vs. more narrative\n",
    "\n",
    "**Real-World Examples:**\n",
    "- **Academic papers**: HIGH density (lots of \"however\", \"therefore\", \"moreover\")\n",
    "- **Children's stories**: LOW density (simple sentences, few transitions)\n",
    "- **News articles**: MEDIUM density (clear but not overly formal)\n",
    "\n",
    "### What This Notebook Does:\n",
    "\n",
    "**Step 1**: Load both Alice books and split them into paragraphs\n",
    "\n",
    "**Step 2**: For each paragraph, count how many discourse markers appear:\n",
    "- We have a list of 19 common markers (like \"however\", \"meanwhile\", \"suddenly\")\n",
    "- We count how many times these words appear in each paragraph\n",
    "- We calculate **density** = number of markers ÷ total words in paragraph\n",
    "\n",
    "**Step 3**: Visualize the results:\n",
    "- **Scatter plots**: See if longer paragraphs use more/fewer markers\n",
    "- **Histograms**: Compare the overall patterns between the two books\n",
    "\n",
    "**Step 4**: Interpret what we find about Carroll's writing style\n",
    "\n",
    "### Connection to Modern AI (ChatGPT, etc.):\n",
    "\n",
    "**How AI Uses This:**\n",
    "\n",
    "When you use ChatGPT or similar AI systems, they understand your text by recognizing patterns like discourse markers. Here's how:\n",
    "\n",
    "1. **During Training**: \n",
    "   - AI learns that \"however\" usually introduces contrast\n",
    "   - \"Therefore\" usually shows logical conclusion\n",
    "   - \"For example\" introduces a specific case\n",
    "   - This helps AI understand the **structure** of arguments and narratives\n",
    "\n",
    "2. **During Use**:\n",
    "   - When you write \"However\" in your prompt, ChatGPT knows you're changing direction\n",
    "   - RAG systems (which search documents to answer questions) can find well-structured paragraphs that use clear markers\n",
    "   - AI-generated text often includes these markers to sound more natural and organized\n",
    "\n",
    "3. **Practical Example**:\n",
    "   - **Without markers**: *\"It's cold. Bring a jacket.\"* (AI might not see the connection)\n",
    "   - **With markers**: *\"It's cold. Therefore, bring a jacket.\"* (AI clearly sees cause → effect)\n",
    "\n",
    "**Why This Matters for Your Learnings:**\n",
    "- If you're building a chatbot, knowing about discourse markers helps you structure responses\n",
    "- If you're analyzing customer reviews, marker density tells you if feedback is casual or structured\n",
    "- If you're working with RAG (document search), you can identify high-quality, well-organized source documents\n",
    "\n",
    "This notebook shows a simple version of what complex AI does automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf695ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_book(filepath: str) -> str:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    if 'CHAPTER I' in text:\n",
    "        start = text.find('CHAPTER I')\n",
    "        text = text[start:]\n",
    "    elif '*** START OF' in text:\n",
    "        start = text.find('*** START OF')\n",
    "        text = text[start + 100:]\n",
    "\n",
    "    if '*** END OF' in text:\n",
    "        end = text.find('*** END OF')\n",
    "        text = text[:end]\n",
    "    elif 'End of Project Gutenberg' in text:\n",
    "        end = text.find('End of Project Gutenberg')\n",
    "        text = text[:end]\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "wonderland_text = load_book('../data/Wonderland.txt')\n",
    "looking_glass_text = load_book('../data/Looking-Glass.txt')\n",
    "\n",
    "print(f\"Wonderland characters: {len(wonderland_text):,}\")\n",
    "print(f\"Looking-Glass characters: {len(looking_glass_text):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01bc8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text: str, min_words: int = 10) -> List[str]:\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    raw_paras = re.split(r'\\n\\s*\\n+', text)\n",
    "    paras = []\n",
    "    for p in raw_paras:\n",
    "        cleaned = re.sub(r'\\s+', ' ', p).strip()\n",
    "        if not cleaned:\n",
    "            continue\n",
    "        if len(cleaned.split()) < min_words:\n",
    "            continue\n",
    "        paras.append(cleaned)\n",
    "    return paras\n",
    "\n",
    "DISCOURSE_MARKERS = [\n",
    "    'however', 'therefore', 'moreover', 'meanwhile', 'suddenly',\n",
    "    'although', 'though', 'even though', 'in contrast', 'on the other hand',\n",
    "    'for example', 'for instance', 'at the same time', 'finally',\n",
    "    'in conclusion', 'nevertheless', 'nonetheless', 'instead', 'after all'\n",
    "]\n",
    "\n",
    "def count_markers(paragraph: str) -> int:\n",
    "    text = paragraph.lower()\n",
    "    count = 0\n",
    "    for m in DISCOURSE_MARKERS:\n",
    "        count += text.count(m)\n",
    "    return count\n",
    "\n",
    "def marker_density(paragraphs: List[str]) -> Tuple[list, list]:\n",
    "    densities = []\n",
    "    lengths = []\n",
    "    for p in paragraphs:\n",
    "        n_words = len(re.findall(r\"\\w+\", p))\n",
    "        if n_words == 0:\n",
    "            continue\n",
    "        markers = count_markers(p)\n",
    "        densities.append(markers / n_words)\n",
    "        lengths.append(n_words)\n",
    "    return densities, lengths\n",
    "\n",
    "wonderland_paras = split_into_paragraphs(wonderland_text)\n",
    "looking_glass_paras = split_into_paragraphs(looking_glass_text)\n",
    "\n",
    "w_dens, w_len = marker_density(wonderland_paras)\n",
    "g_dens, g_len = marker_density(looking_glass_paras)\n",
    "\n",
    "print(f\"Wonderland mean discourse markers per word: {sum(w_dens)/len(w_dens):.5f}\")\n",
    "print(f\"Looking-Glass mean discourse markers per word: {sum(g_dens)/len(g_dens):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540976dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "axes[0].scatter(w_len, w_dens, alpha=0.5)\n",
    "axes[0].set_title('Wonderland')\n",
    "axes[0].set_xlabel('Paragraph length (words)')\n",
    "axes[0].set_ylabel('Discourse marker density')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(g_len, g_dens, alpha=0.5)\n",
    "axes[1].set_title('Looking-Glass')\n",
    "axes[1].set_xlabel('Paragraph length (words)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe21e9f",
   "metadata": {},
   "source": [
    "Why are several different paths visible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(w_dens, bins=20, alpha=0.6, label='Wonderland', density=True)\n",
    "ax.hist(g_dens, bins=20, alpha=0.6, label='Looking-Glass', density=True)\n",
    "ax.set_xlabel('Discourse marker density (markers per word)')\n",
    "ax.set_ylabel('Density (normalized)')\n",
    "ax.set_title('Distribution of Discourse Marker Density')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f00c0",
   "metadata": {},
   "source": [
    "#### Histogram (Distribution Comparison)\n",
    "\n",
    "**What you're seeing:**\n",
    "- This shows HOW COMMON different density levels are\n",
    "- **X-axis**: Marker density ranges\n",
    "- **Y-axis**: How frequently that density occurs (normalized so both books are comparable)\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Tall Peak Near Zero**:\n",
    "   - The huge spike at the left (around 0.00) means MOST paragraphs have zero or very few markers\n",
    "   - Both books are dominated by simple, direct narrative prose\n",
    "\n",
    "2. **Long Right Tail**:\n",
    "   - The gradual decline to the right shows that higher densities are increasingly rare\n",
    "   - Very few paragraphs exceed 0.04 density\n",
    "\n",
    "3. **Both Books Overlap Almost Perfectly**:\n",
    "   - The blue (Wonderland) and orange (Looking-Glass) distributions are nearly identical\n",
    "   - This confirms Carroll had a very consistent writing style\n",
    "\n",
    "**What Would Be Different?**\n",
    "- **Academic paper**: Would have a peak around 0.02-0.04 (much higher)\n",
    "- **News article**: Peak around 0.01-0.02 (moderate)\n",
    "- **Children's book (like this)**: Peak at 0.00-0.01 (low, as we see)\n",
    "\n",
    "---\n",
    "\n",
    "### What This Tells Us About Lewis Carroll's Writing\n",
    "\n",
    "1. **Narrative-First Style**: Carroll focuses on storytelling through action and dialogue rather than explicit logical connections\n",
    "\n",
    "2. **Accessible Language**: Low marker density makes the text easier to read - perfect for his young audience\n",
    "\n",
    "3. **Consistent Voice**: Both books show identical patterns, suggesting a deliberate, polished style\n",
    "\n",
    "4. **Strategic Marker Use**: The few high-density paragraphs likely mark important moments where Carroll wants to emphasize logical connections or contrasts\n",
    "\n",
    "### Application to Your Own Projects\n",
    "\n",
    "**If you're analyzing text:**\n",
    "- High marker density → formal, structured, argumentative\n",
    "- Low marker density → narrative, conversational, descriptive\n",
    "- Use this to automatically classify text type or writing style\n",
    "\n",
    "**If you're generating text (with AI or manually):**\n",
    "- Fiction/stories: Keep density low (like Carroll)\n",
    "- Explanations/tutorials: Use moderate density for clarity\n",
    "- Academic/business: Use higher density for professional tone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971becbe",
   "metadata": {},
   "source": [
    "## Memory Cleanup\n",
    "\n",
    "If you're running low on memory, run this cell to free up RAM by deleting large variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete large variables to free memory\n",
    "del wonderland_text, looking_glass_text\n",
    "del wonderland_paras, looking_glass_paras\n",
    "del w_dens, w_len, g_dens, g_len\n",
    "\n",
    "# Clear matplotlib figures\n",
    "plt.close('all')\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory cleaned! Large variables deleted and garbage collected.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
